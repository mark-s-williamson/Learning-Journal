# Building Robust Data Pipelines

    Topic 1 - Linux for Data Engineers<br/>
	Use shell scripts to automate data extraction tasks and schedule cron jobs for regular data processing.<br/>
    Topic 2 - Version control<br/>
	Maintaining a Git repository for your ETL scripts, enabling rollback to previous versions if issues arise.<br/>
    Topic 3 - Python for Data Engineers<br/>
	Writing Python scripts to clean and transform raw data before loading it into a data warehouse.<br/>
    Topic 4 - Data structures and OOP<br/>
	Implementing classes and methods to modularise and reuse code across different stages of your pipeline.<br/>
    Topic 5 - Data manipulation<br/>
	Using Pandas to filter, aggregate, and pivot data before loading it into a reporting database.<br/>
    Topic 6 - Algorithmic thinking<br/>
	Designing efficient algorithms for data deduplication and record linkage.<br/>
    Topic 7 - Parallel programming<br/>
	Implementing multi-threading or multi-processing in Python to handle large data volumes more efficiently.<br/>
    Topic 8 - Spark for data engineers<br/>
	Writing Spark jobs to perform data aggregation and transformation on a cluster.<br/>
	
## New Framework: Docker
Docker moves packaging applications and their dependencies into containers, ensuring consistent environments across different stages of development and deployment.<br/>
Programonaut.com to Dockerise code

## New Framework: Kubernetes
Similar container system to Docker, but also self heals and scales horizontally.

# Making your data pipelines more secure
### Input Validation 
No more Little Bobby Tables.
### Encryption
Transport Layer Security (TLS) on data transmission and http**s** for encrypted traffic.
### Authentication and authorisation
Multi Factor Authentication (MFA) and Role Based Access Control (RBAC)
### Regular Security Audits
### Monitoring and Logging
Security Information and Event Management (SIEM)
### Data Masking



# Utilising data visualisation for impact

## Python Library: Seaborn
provides a high-level interface for drawing attractive and informative statistical graphics.

## Python Library: Plotly
for creating interactive plots. It allows you to build complex visualisations that users can interact with in real-time.

# Writing effective test suites
## Unit Testing
testing individual components of your application in isolation
## Integration Testing
combining individual units or components of your application and testing them as a group.<br/>
CI/CD pipelines: Use tools like Jenkins, GitLab CI, or Travis CI to automate integration tests as part of your continuous integration and continuous deployment (CI/CD) pipeline.<br/>
Testing frameworks: Leverage frameworks like pytest for Python, which supports integration testing with fixtures and plugins for simulating complex environments.<br/>
Monitoring and logging: Implement monitoring and logging solutions like ELK Stack (Elasticsearch, Logstash, Kibana) to track test execution and diagnose issues.<br/>

# Module consolidation
    Example: Creating a portfolio project that demonstrates your ability to build a robust and secure data pipeline using Docker, Kubernetes, and secure coding practices.<br/>
    Benefits: A well-documented portfolio enhances your credibility and can open up new career opportunities.<br/>


