# The Strategic Value of Big Data
## Chief Decision Officers and Chief Data Officers
* These positions leverage the wealth of data to inform decision-making processes.
* Highlighting the potential to find new uses for old data.

## Data Marketplaces
* **AWS Data Exchange** - AWS Data Exchange is a data marketplace hosted by Amazon Web Services (AWS). It offers a wide range of third-party data products from providers such as Reuters, Foursquare, and Dun & Bradstreet. Users can subscribe to datasets and integrate them directly into their AWS environments for analysis and processing.
* **Kaggle** - Kaggle, a subsidiary of Google, hosts a diverse collection of datasets contributed by the global data science community. These datasets cover various topics, including machine learning, healthcare, finance, and more. Users can explore and download datasets for free or participate in data science competitions hosted on the platform.
* **Data World** - Data.world is a collaborative platform for data sharing and analysis. It hosts a vast library of public datasets contributed by individuals, organisations, and government agencies. Users can discover, explore, and analyse datasets using built-in tools and collaborate with others on data projects.


# Storing Files: HDFS, Key-Value Stores, Columnar â€“ Parquet Format, Filesystems

## Hadoop Distributed File System (HDFS) 
Offers resilience by storing three copies of each file and distribution by ensuring each file resides on at least three different machines. This configuration not only provided a cost-effective solution for managing vast amounts of data but also enhanced data accessibility and analysis capabilities.

## Key-Value Stores
Offers a straightforward yet efficient method for storing and retrieving data based on a unique key, tailoring their approach for scenarios where quick data access is critical. 

## Parquet Format
Designed for efficiency in analytic queries, employs a columnar storage strategy that significantly reduces disk I/O, facilitating faster data processing and enabling schema evolution with minimal overhead.

## Storage Format
HDDs, with their mechanical parts, are generally more affordable but slower and more prone to failure compared to SSDs, which offer faster data access but at a higher price point. The Parquet format, by addressing the need for efficient data analytics through its columnar storage approach, enables organisations to conduct analytic queries more efficiently, leveraging schema evolution to adapt to changing data requirements without significant reengineering.

## How does Parquet file format differ from CSV and JSON files
### Structure:     
CSV files are plain text files where each line represents a single row of data, and values are separated by commas or other delimiters. <br/>JSON files store data in a hierarchical format using key-value pairs, making them suitable for representing nested or complex data structures. <br/>Parquet files are columnar storage formats, meaning data is stored by column rather than by row. This structure allows for better compression and efficient storage, especially for large datasets with many columns
### Efficiency:    
CSV files are simple and easy to read/write but may not be efficient for large datasets or analytical workloads due to their row-oriented structure.<br/> JSON files offer flexibility and support for complex data structures but can be less efficient in terms of storage and processing compared to more optimized formats.<br/> Parquet files are highly optimized for analytical queries, as they allow for selective column reads and efficient compression techniques. This results in faster data processing and reduced I/O overhead compared to CSV and JSON files.
### Functionality:     
CSV files are widely supported and easy to work with in many programming languages and tools. However, they lack built-in support for nested data structures or data types.<br/> JSON files support nested structures and are commonly used for representing semi-structured data. They are well-suited for web APIs and applications where flexibility is important.<br/> Parquet files offer advanced features such as schema evolution, predicate pushdown, and efficient encoding schemes. These features make Parquet ideal for data analytics and big data processing frameworks like Apache Spark and Apache Hive

# Data Lakes, Data Warehouses, and Operational Data Stores

## Data Lakes
Vast reservoirs of raw, unstructured data. <br/>
Unparalleled flexibility in data storage, allowing businesses to store data in its native format without the need for upfront schema definition

## Data Warehouses
Highly structured repositories designed to facilitate efficient data analysis and reporting

## Operational Data Stores
Designed for the real-time processing of transactional data, supporting day-to-day business operations<br/>
A bridge between transactional databases and data warehouses or data lakes

# Data Structuring: Schema-on-Write, Relational Modelling
## Schema on Write
The schema, or structure of the data, must be defined before any data is written to the database.

## Schema on Read vs Schema on write
Schema on Write	/ Schema on Read<br/>
Fast reads      / Slower reads<br/>
Slower loads    / Fast loads<br/>
Not agile       / Very agile<br/>
Structured      / Structured / Unstructured <br/>
Fewer errors    / More errors<br/>

  
###

