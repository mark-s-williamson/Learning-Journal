# Establishing Data Quality SLAs
- Accuracy: Ensuring data correctly represents real-life entities.
- Completeness: Making sure all required data is present.
- Consistency: Maintaining uniformity in data representation.
- Integrity: Ensuring data is complete, accurate, and consistent.
- Reasonability: Checking if data patterns meet expectations.
- Timeliness: Ensuring data is up-to-date and available when needed.
- Uniqueness/Deduplication: Ensuring no entity exists more than once
- Validity: Ensuring data values are within defined domains.

## Data Quality SLA elements
- Data elements covered by the agreement.
- Business impacts associated with data flaws.â€¢
- Data quality dimensions associated with each data element (see previous lesson).
- Expectations for quality for each data element for each of the identified dimensions in each application or system in the data value chain.
- Methods for measuring against those expectations.
- Acceptability threshold for each measurement.
- Data Steward(s) to be notified in case the acceptability threshold is not met.
- Timelines and deadlines for expected resolution or remediation of the issue.
- Escalation strategy, and possible rewards and penalties.

## Data Quality Reporting

- Data quality scorecard, which provides a high-level view of the scores associated with various metrics, reported to different levels of the organisation within established thresholds.
- Data quality trends, which show over time how the quality of data is measured, and whether trending is up or down.
- SLA Metrics, such as whether operational data quality staff diagnose and respond to data quality incidents in a timely manner.
- Data quality issue management, which monitors the status of issues and resolutions.
- Conformance of the Data Quality team to governance policies.
- Conformance of IT and business teams to Data Quality policies.
- Positive effects of improvement projects.

---

# Data Quality Techniques
## Preventative and Corrective
| Preventative               		| Corrective                       	|
|--------------------------			|----------------------------------	|
| Data Entry Controls           	|    Automated Correction 			| 
| Data Producer Training        	| Manually Directed Correction 		|
| Rule Definition         			|     Manual Correction            	|
| Demanding HQ data from suppliers  | 									|
| Data Governance and Stewardship   |  									|
| Formal Change Control             |  									|

## Statistical Process Control (SPC)
A process is a series of steps executed to turn inputs into outputs.<br/>
 SPC is based on the assumption that when a process with consistent inputs is executed consistently, it will produce consistent outputs. <br/>
 It uses measures of central tendency (how values cluster around a central value, such as a mean, median, or mode) and of variability around a central value (e.g., range, variance, standard deviation), to establish tolerances for variation within a process.<br/> 
 The primary tool used for SPC is the control chart
 
 ### Process Variation Types:
 ** Common Causes
 ** Special Causes

## Root Cause Analysis
Root cause analysis is a process of understanding factors that contribute to problems and the ways they contribute. Its purpose is to identify underlying conditions that, if eliminated, would mean problems would disappear.<br/>

--- 

# Strategies for Ingestion PII and Sensitive Data
## Personally Identifiable Information

- Data Discovery and Classification
- Data Minimsation
- Secure Data Storage
- Compliance and Policy Implementation
- Regular Audits and Monitoring
- Employee Training

## Sensitive vs Non Sensitive PII
| Type              		| Example                      	|
|--------------------------			|----------------------------------	|
| Sensitive        	|    National Insurance, Financial Details, Health Records, Biometric Identification, Personal Identification Numbers (PINs)| 
| Non-Sensitive        	|    Names, Addresses, Phone Numbers, Email Addresses, Usernames | 

- **Application Programming Interfaces:** Allow two or more applications to communicate by providing a set of procedures and functions.
- **MicroServices** Small, independent blocks within the data engineering ecosystem



---

