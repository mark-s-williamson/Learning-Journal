# Defining your data ingestion architecture

## Two Main Types of data ingestion

- **Batch Processing**
- **Real-Time/Streaming**

### Differences

- **Timing**: Batch: Scheduled. Real-Time: Continously
- **Suitability**: Batch: Large volume at once. Real-Time: Immediate data processing and quick decision making.
- **Characteristics**: Batch: Simpler and less resource intensive. Higher latency, less relevance. Real-Time: Complex and resource intensive. Requires higher tech.
- **Use Cases**: Batch: Data doesn't change rapidly. Real-Time: Real Time analytics and monitoring.

## Parameters for designing a data ingestion solution
- **Data Velocity**
- **Data Volume**
- **Data Frequency**
- **Data Format**
- **Data Security and Governance**
- **Monitoring and Tracking**

---

# Advanced data ingestion patterns

## Strategies for ETL from diverse sources
- Where does the data originate?
- Where is it going to be stored?
- How is it currently structured?


### Change Data Capture (CDC)
Change Data Capture (CDC) is a design pattern that captures incremental changes made to a database and applies them to a target data store in near real-time.<br/>
CDC uses transaction logs or triggers to identify and capture changes.<br/>
**Useful for** 
- synchronise data between transactional and analytic systems
- enable real-time analytics on operational data
- Feed real-time dashboards
- Replicate data for disaster recovery
**CDC pipelines require**
- Source database with CDC enabled (mySQL, postgres, Oracle)
- CDC tool (AWS Database Migration Service, Azure Data Factory, Google Cloud Datastream)
- Dashboard/application output
- Target datastore (warehouse/lake/NoSQL)


### Lambda Architecture
combines batch and real-time processing to enable comprehensive and accurate data analysis. <br/>
It consists of three layers: batch, speed, and serving. <br/>
**Useful for** 
- Use cases with both real-time and historical data analysis
- Handle complex transformations/aggregations
- Ensuring data consistency across both batch and real-time views
- Support ad-hoc queries
**Lambda Architectures require**
- Batch layer: ingests and processes historical data (Hadoop, Spark).
- Speed layer: ingests and processes real-time data (Spark Streaming, Flink).
- Serving layer: combines batch and real-time views and serves data to applications (Cassandra, HBase).
**Lambda Tools**
- Open Source: Apache Hadoop, Apache Spark, Apache Cassandra
- Cloud-Specific: AWS EMR, Azure HDInsight, Google Cloud DataProc
- Cloud-agnostic: Databricks


### Kappa Architecture
simplification of the Lambda Architecture that uses a single, real-time processing pipeline for both real-time and historical data.
**Useful for** 
- Use cases that require real-time data processing
- Simplfying data architecture by removing batch
- Enable faster data processing and analysis
**Lambda Architectures require**
- Message queue or log: To capture all data (Kafka, Kinesis).
- Stream processing engine: To process and serve data (Spark Streaming, Flink).
- Serving layer: To store and expose processed data (Cassandra, HBase).
**Lambda Tools**
- Open-source: Apache Kafka, Apache Flink, Apache Druid.
- Cloud-agnostic: Confluent Platform, Materialise.


| Criteria                | Lambda                                   | Kappa                                         |
|--------------------------|------------------------------------------|-----------------------------------------------|
| Processing paradigm      | Batch & Streaming                        | Streaming                                     |
| Re-processing paradigm   | Every Batch Cycle                        | Only when code changes                        |
| Resource consumption     | Function = Query (All data)              | Incremental algorithms, running on deltas     |
| Reliability              | Batch is reliable, Streaming is approximate | Streaming with consistency (exactly once) |


---

# Cloud Ingestion Use Cases

### AWS Glue Workflow:
- **Amazon Simple Storage Solution (S3)** - Stores data
- **AWS Lambda** - Triggers one or more Glue ETL jobs
- **AWS Glue ETL** - ETL jobs
- **AWS S3 or Amazon Redshift** - Stores data after ETL
- **AWS Glue Data Catalog** - Stores metadata
- **AWS Cloudwatch** - logs and notifications


### Azure Workflow:
- **Amazon (S3), Azure Blob, Azure Synapse, Google Bigquery, Oracle** - Data Warehouse / ETL
- **Azure Active Directory** - Analysis
- **Power BI** Visualisation
- **AWS Glue Data Catalog** - Stores metadata


---



